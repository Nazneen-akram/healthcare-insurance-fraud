{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvq/W70h9LChaJ10bh5pn3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nazneen-akram/healthcare-insurance-fraud/blob/main/initial_EDA_Fraud_Claims_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pYhsfFGQc1fq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Functions to join Dataset\n",
        "##**'Beneficiary'**, **'Inpatient'**, **'Outpatient'** and **'Fraud'** into **'merged.csv'**\n",
        "\n",
        "1.   **join_inpatient_outpatient():** : Function to merge inpatient, outpatient dataframe\n",
        "2.   **join_inpatient_outpatient_beneficiary()**: Function to merge merged df, beneficiary dataframe\n",
        "3. **join_inpatient_outpatient_beneficiary_fraud()**: Function to merge merged df, fraud dataframe\n",
        "4. **join_csv()**: Function to join csv files. Joining by key BeneID, Provider.\n",
        "5. **generate_merged_data()**: mainfunction to join csv files. calls read_data and join_csv functions\n",
        "\n"
      ],
      "metadata": {
        "id": "f9L6l3CR9XEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data():\n",
        "    \"\"\"\n",
        "    function to read csv files\n",
        "    parameters: None\n",
        "    return: data frames fraud, beneficiary, inpatient, outpatient.\n",
        "    raise FileExistsError: raises an exception when file is not found\n",
        "    \"\"\"\n",
        "    try:\n",
        "        fraud=pd.read_csv(\"/content/Train-1542865627584.csv\")\n",
        "        beneficiary=pd.read_csv(\"/content/Train_Beneficiarydata-1542865627584.csv\")\n",
        "        inpatient=pd.read_csv(\"/content/Train_Inpatientdata-1542865627584.csv\")\n",
        "        outpatient=pd.read_csv(\"/content/Train_Outpatientdata-1542865627584.csv\")\n",
        "        return fraud, beneficiary, inpatient, outpatient\n",
        "    except FileExistsError as error:\n",
        "        raise error"
      ],
      "metadata": {
        "id": "eH_7by3H5ObK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def join_inpatient_outpatient(inpatient, outpatient):\n",
        "    \"\"\"\n",
        "    function to merge inpatient, outpatient dataframe\n",
        "    parameters: inpatient, outpatient\n",
        "    return: merged dataframe\n",
        "    \"\"\"\n",
        "    inpatient['is_Inpatient'] = 1\n",
        "    outpatient['is_Inpatient'] = 0\n",
        "    inpatient_outpatient = pd.concat([inpatient,outpatient])\n",
        "    return inpatient_outpatient\n",
        "\n",
        "\n",
        "def join_inpatient_outpatient_beneficiary(inpatient_outpatient, beneficiary):\n",
        "    \"\"\"\n",
        "    function to merge merged df, beneficiary dataframe\n",
        "    parameters: inpatient, outpatient merged and beneficiary\n",
        "    return: merged dataframe\n",
        "    \"\"\"\n",
        "    inpatient_outpatient_beneficiary=pd.merge(inpatient_outpatient,beneficiary,\n",
        "                                left_on='BeneID',right_on='BeneID',how='inner')\n",
        "    return inpatient_outpatient_beneficiary\n",
        "\n",
        "\n",
        "def join_inpatient_outpatient_beneficiary_fraud(inpatient_outpatient_beneficiary, fraud):\n",
        "    \"\"\"\n",
        "    function to merge merged df, fraud dataframe\n",
        "    parameters: inpatient, outpatient merged, beneficiary and fraud\n",
        "    return: merged dataframe\n",
        "    \"\"\"\n",
        "    inpatient_outpatient_beneficiary_fraud=pd.merge(fraud,inpatient_outpatient_beneficiary,\n",
        "                                                    on='Provider')\n",
        "    return inpatient_outpatient_beneficiary_fraud\n",
        "\n",
        "\n",
        "def join_csv(fraud, beneficiary, inpatient, outpatient):\n",
        "    \"\"\"\n",
        "    function to join csv files. Joining by key BeneID, Provider.\n",
        "    parameters: fraud, beneficiary, inpatient, outpatient dataframes\n",
        "    return: merged csv\n",
        "    \"\"\"\n",
        "    # Join the files\n",
        "    merged = join_inpatient_outpatient(inpatient, outpatient)\n",
        "    merged = join_inpatient_outpatient_beneficiary(merged, beneficiary)\n",
        "    merged = join_inpatient_outpatient_beneficiary_fraud(merged, fraud)\n",
        "\n",
        "    # Save the merged file as a CSV\n",
        "    merged.to_csv('/content/merged.csv', index=False)\n",
        "\n",
        "\n",
        "def generate_merged_data():\n",
        "    \"\"\"\n",
        "    mainfunction to join csv files. calls read_data and join_csv functions\n",
        "    parameters: None\n",
        "    return: None\n",
        "    \"\"\"\n",
        "    fraud, beneficiary, inpatient, outpatient = read_data()\n",
        "    join_csv(fraud, beneficiary, inpatient, outpatient)\n",
        "\n"
      ],
      "metadata": {
        "id": "qgkm6cIZ8PxA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_merged_data()"
      ],
      "metadata": {
        "id": "ZgYi3wXS8Tnb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VXgGGkCD8WeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Pre-processing**\n",
        "###Module to perform data preprocesssing. This will be first level preprocessing. For visualization\n",
        "###and machine learning modeling seperate preprocessing will be required according to the requirements."
      ],
      "metadata": {
        "id": "028uKFwz_FLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "9sOI7yZ__LzS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data():\n",
        "    \"\"\"\n",
        "    function to read csv file\n",
        "    parameters: None\n",
        "    return: data frames fraud, beneficiary, inpatient, outpatient.\n",
        "    raise FileExistsError: raises an exception when file is not found\n",
        "    \"\"\"\n",
        "    try:\n",
        "        merged = pd.read_csv(\"/content/merged.csv\",low_memory=False)\n",
        "    except FileExistsError as error:\n",
        "        raise error\n",
        "    return merged"
      ],
      "metadata": {
        "id": "OKRb2qm5_OUQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoding_potential_fraud(dataframe):\n",
        "    \"\"\"\n",
        "    function encode catagorical field having yes/no into a numerical field\n",
        "    parameters: merged dataset\n",
        "    return: modified dataframe with potencialfraud encoded into numerical value\n",
        "    \"\"\"\n",
        "    dataframe['PotentialFraud'] = dataframe['PotentialFraud'].map({'Yes': 1, 'No': 0})\n",
        "    # corr_to_target = dataframe.corr()['PotentialFraud'].abs().sort_values(ascending=False)\n",
        "    return dataframe\n",
        "\n",
        "\n",
        "def encode_categorical_data(dataframe):\n",
        "    \"\"\"\n",
        "    Encode categorical fields with 'yes/no' into numerical fields.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame to process.\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame with categorical columns encoded into numerical values.\n",
        "    \"\"\"\n",
        "\n",
        "    # Assuming '2' should be replaced with '0' for the specified chronic conditions.\n",
        "    chronic_conditions = [\n",
        "        'ChronicCond_Alzheimer', 'ChronicCond_Heartfailure', 'ChronicCond_KidneyDisease',\n",
        "        'ChronicCond_Cancer', 'ChronicCond_ObstrPulmonary', 'ChronicCond_Depression',\n",
        "        'ChronicCond_Diabetes', 'ChronicCond_IschemicHeart', 'ChronicCond_Osteoporasis',\n",
        "        'ChronicCond_rheumatoidarthritis', 'ChronicCond_stroke'\n",
        "    ]\n",
        "    for condition in chronic_conditions:\n",
        "        dataframe[condition] = dataframe[condition].replace(2, 0)\n",
        "\n",
        "    # Replace 'Y' with '1' for 'RenalDiseaseIndicator'.\n",
        "    dataframe['RenalDiseaseIndicator'] = dataframe['RenalDiseaseIndicator'].replace('Y', 1)\n",
        "\n",
        "    return dataframe\n",
        "\n",
        "\n",
        "\n",
        "def add_admit_column(dataframe):\n",
        "    \"\"\"\n",
        "    function to add a column admitdays\n",
        "    parameters: merged dataset\n",
        "    return: modified dataframe with added admitfordays column\n",
        "    \"\"\"\n",
        "    dataframe['AdmissionDt'] = pd.to_datetime(dataframe['AdmissionDt'], format='%Y-%m-%d')\n",
        "    dataframe['DischargeDt'] = pd.to_datetime(dataframe['DischargeDt'], format='%Y-%m-%d')\n",
        "    dataframe['AdmitForDays'] = ((dataframe['DischargeDt'] - dataframe['AdmissionDt']).dt.days) + 1\n",
        "    return dataframe\n",
        "\n",
        "\n",
        "def add_age_column(dataframe):\n",
        "    \"\"\"\n",
        "    function to add a column age based on DOB and DOD\n",
        "    parameters: merged dataset\n",
        "    return: modified dataframe with added age column\n",
        "    \"\"\"\n",
        "    dataframe['DOB'] = pd.to_datetime(dataframe['DOB'], format='%Y-%m-%d')\n",
        "    dataframe['DOD'] = pd.to_datetime(dataframe['DOD'], format='%Y-%m-%d', errors='ignore')\n",
        "    dataframe['Age'] = round(((dataframe['DOD'] - dataframe['DOB']).dt.days) / 365)\n",
        "    dataframe.Age.fillna(round(((pd.to_datetime('2009-12-01', format='%Y-%m-%d') -\n",
        "                                 dataframe['DOB']).dt.days) / 365),\n",
        "                         inplace=True)\n",
        "\n",
        "    dataframe.Age.fillna(round(((pd.to_datetime('2009-12-01', format='%Y-%m-%d') -\n",
        "                                 dataframe['DOB']).dt.days) / 365),\n",
        "                         inplace=True)\n",
        "    return dataframe\n",
        "\n",
        "\n",
        "def adding_dead_column(dataframe):\n",
        "    \"\"\"\n",
        "    function to add a column to get if the person is dead or not\n",
        "    parameters: merged dataset\n",
        "    return: modified dataframe with added WhetherDead column\n",
        "    \"\"\"\n",
        "    dataframe.loc[dataframe.DOD.isna(), 'WhetherDead'] = 0\n",
        "    dataframe.loc[dataframe.DOD.notna(), 'WhetherDead'] = 1\n",
        "    dataframe.loc[:, 'WhetherDead'].head(7)\n",
        "    return dataframe\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "def create_columns_visualization(df, state_mapping):\n",
        "    \"\"\"\n",
        "    Adds columns to indicate patient status, age, state, admission duration, and insurance claim reimbursement buckets.\n",
        "\n",
        "    Parameters:\n",
        "    - df: The input DataFrame.\n",
        "    - state_mapping: DataFrame containing state mappings.\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame with new columns added.\n",
        "    \"\"\"\n",
        "    # Filter for inpatient records\n",
        "    inpatient_df = df[df['is_Inpatient'] == 1]\n",
        "\n",
        "    # Convert date columns to datetime format\n",
        "    for col in ['ClaimStartDt', 'ClaimEndDt']:\n",
        "        inpatient_df[col] = pd.to_datetime(inpatient_df[col], format='%Y-%m-%d')\n",
        "\n",
        "    # Calculate age based on a fixed date\n",
        "    inpatient_df['Age'] = inpatient_df['DOB'].apply(lambda x: 2013 - x.year if pd.notnull(x) else None)\n",
        "\n",
        "\n",
        "    # Merge with state mapping\n",
        "    inpatient_df = inpatient_df.merge(state_mapping, on='State', how='left')\n",
        "\n",
        "    # Admission day buckets\n",
        "    inpatient_df['Days_Admitted_Bucket'] = inpatient_df['AdmitForDays'].apply(\n",
        "        lambda x: \"0-20 Days\" if x <= 20 else \"More than 20 Days\")\n",
        "\n",
        "    # Insurance claim amount reimbursement buckets\n",
        "    def reimbursement_bucket(x):\n",
        "        if x <= 20000:\n",
        "            return '0 - 20000'\n",
        "        elif 20000 < x <= 40000:\n",
        "            return '20000 - 40000'\n",
        "        elif 40000 < x <= 60000:\n",
        "            return '40000 - 60000'\n",
        "        else:\n",
        "            return 'Greater than 60000'\n",
        "\n",
        "    inpatient_df['InscClaimAmtReimbursed_Bucket'] = inpatient_df['InscClaimAmtReimbursed'].apply(reimbursement_bucket)\n",
        "\n",
        "    return inpatient_df\n",
        "\n",
        "\n",
        "def save_csv(dataframe):\n",
        "    \"\"\"\n",
        "    function to save preprocessed data into a csv file, to be used for further\n",
        "    computations of the project.\n",
        "    parameters: merged dataset\n",
        "    return: None\n",
        "    \"\"\"\n",
        "    dataframe.to_csv('/content/preprocessed.csv', index=False)\n",
        "\n",
        "\n",
        "def save_test_data(x_test,y_test):\n",
        "    \"\"\"\n",
        "    function to save test data into a csv file, to be used for further\n",
        "    computations of the project.\n",
        "    parameters: merged dataset\n",
        "    return: None\n",
        "    \"\"\"\n",
        "    x_test.to_csv('/content/test.csv', index=False)\n",
        "    y_test.to_csv('/content/test_labels.csv', index=False)\n",
        "\n",
        "\n",
        "def pre_processing():\n",
        "    \"\"\"\n",
        "    function to read and then preprocess data\n",
        "    parameters: merged dataset\n",
        "    return: modified dataframe with added WhetherDead column\n",
        "    \"\"\"\n",
        "    dataframe = read_data()\n",
        "    dataframe = encode_categorical_data(dataframe)\n",
        "    dataframe = add_admit_column(dataframe)\n",
        "    dataframe = add_age_column(dataframe)\n",
        "    dataframe = adding_dead_column(dataframe)\n",
        "    dataframe = encoding_potential_fraud(dataframe)\n",
        "    return dataframe\n"
      ],
      "metadata": {
        "id": "jXdrDd6i_Sqt"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main_processing():\n",
        "    \"\"\"\n",
        "    Main function to preprocess data, create visualizations, split data into training and testing sets,\n",
        "    and save processed data to CSV.\n",
        "    \"\"\"\n",
        "    # Preprocess data and save initial CSV\n",
        "    df = pre_processing()\n",
        "    save_csv(df)\n",
        "\n",
        "    # Load state mapping and create visualization data\n",
        "    state_mapping = pd.read_csv(\"/content/State_Mapping.csv\")\n",
        "    viz_df = create_columns_visualization(df, state_mapping)\n",
        "    viz_df.to_csv('/content/visualization.csv', index=False)\n",
        "\n",
        "    # Split data into features and labels\n",
        "    features, labels = df.drop(\"PotentialFraud\", axis=1), df['PotentialFraud']\n",
        "\n",
        "    # Train-test split\n",
        "    x_train, x_test, y_train, y_test = train_test_split(features, labels, random_state=1, test_size=0.10, shuffle=True)\n",
        "\n",
        "    # Process training data\n",
        "    training_data = process_data(x_train, y_train, '/content/training_data.csv')\n",
        "\n",
        "    # Process testing data\n",
        "    test_data = process_data(x_test, y_test, '/content/testing_data.csv', is_train=False)\n",
        "\n",
        "def process_data(x, y=None, filepath='/content/data.csv', is_train=True):\n",
        "    \"\"\"\n",
        "    Process data by selecting numeric types, filling NA values, and optionally concatenating labels.\n",
        "\n",
        "    Parameters:\n",
        "    - x: Features DataFrame.\n",
        "    - y: Labels Series (optional).\n",
        "    - filepath: Path to save processed data.\n",
        "    - is_train: Boolean indicating if the data is training data.\n",
        "\n",
        "    Returns:\n",
        "    - Processed DataFrame.\n",
        "    \"\"\"\n",
        "    # Select numeric columns and fill NA values\n",
        "    x_numeric = x.select_dtypes(exclude=['object', 'datetime64[ns]']).fillna(0)\n",
        "\n",
        "    if is_train:\n",
        "        # Concatenate labels for training data\n",
        "        data = pd.concat([x_numeric, y], axis=1)\n",
        "    else:\n",
        "        data = x_numeric\n",
        "\n",
        "    # Save processed data to CSV\n",
        "    data.to_csv(filepath, index=False)\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "Ib90UqBdBLlL"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_processing()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6OiYSgMDbXR",
        "outputId": "3b8bf596-2983-483b-ebed-33aee5e3d8bc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-9c7e7dce598d>:101: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  inpatient_df[col] = pd.to_datetime(inpatient_df[col], format='%Y-%m-%d')\n",
            "<ipython-input-21-9c7e7dce598d>:101: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  inpatient_df[col] = pd.to_datetime(inpatient_df[col], format='%Y-%m-%d')\n",
            "<ipython-input-21-9c7e7dce598d>:104: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  inpatient_df['Age'] = inpatient_df['DOB'].apply(lambda x: 2013 - x.year if pd.notnull(x) else None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9upaQEbVDdOY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}